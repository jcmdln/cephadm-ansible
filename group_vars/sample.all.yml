# all.sample.yml
#
# vars are in groups, both in alpha-numerical order.
#
##

#
# Config
#

# TODO: Clarify how this works, probably with big warning notices
cephadm_config:
  # Stage to run _AFTER_
  bootstrap:
    # ceph osd
    osd:
      # ceph osd erasure-code-profile set
      erasure-code-profile:
        # ceph osd erasure-code-profile set host-jerasure-3-2 ...
        - name: host-jerasure-3-2
          crush_failure_domain: host
          plugin: jerasure
          k: 3
          m: 2

        # ceph osd erasure-code-profile set rack-isa-8-3 ...
        - name: rack-isa-8-3
          crush_failure_domain: rack
          plugin: isa
          technique: reed_sol_van
          k: 8
          m: 3

#
# Dashboard
#

# Whether the dashboard should be deployed.
cephadm_dashboard: true

# The username and password to use for the administrative user
cephadm_dashboard_password: admin
cephadm_dashboard_username: admin

# The IP address and port(s) to use for the dashboard
cephadm_dashboard_server_addr: >-
  "{{ hostvars[inventory_hostname].ansible_default_ipv4.address }}"
cephadm_dashboard_server_port: 8080
cephadm_dashboard_server_port_ssl: 8443

# Add your own certs to the dashboard, if desired.  Maybe I should
# see about using LetsEncrypt to make/manage SSLs across the
# deployment.
cephadm_dashboard_ssl_crt: ""
cephadm_dashboard_ssl_key: ""

#
# Firewall
#

# Placeholder
cephadm_firewall_firewalld: true

#
# Health
#

# Placeholder
cephadm_health_mon_check: ""

#
# Host
#

# Packages that we install using the host package manager.  Right now
# this is specific to Ubuntu 18.04 LTS, but later I'll hide these bits
# in a role and "automagically" figure out what should be in place,
# unless this variable is defined by you.
cephadm_host_packages:
  - ceph-common
  - cephadm
  - docker.io
  - python3-pip
  - python3-virtualenv

# Same with the above.
cephadm_host_services:
  - docker

#
# Mon
#

# Placeholder
cephadm_mon: ""

#
# Network
#

#
# Values: ipv4, ipv6
cephadm_network_ip_version: ipv4

# The network interface assignments you want to make.  At minimum you
# must define your cluster interface, and the others will simply also
# use it.
cephadm_network_interface_cluster: eno1
cephadm_network_interface_public: "{{ cephadm_network_interface_cluster }}"

cephadm_network_interface_mgrs: "{{ cephadm_network_interface_cluster }}"
cephadm_network_interface_mons: "{{ cephadm_network_interface_public }}"
cephadm_network_interface_osds: "{{ cephadm_network_interface_cluster }}"
cephadm_network_interface_rgws: "{{ cephadm_network_interface_public }}"

#
# OSD
#

# https://docs.ceph.com/docs/octopus/cephadm/drivegroups/
cephadm_osd_drivegroups:
  # group name
  default:
    # hosts to match against
    host_pattern: "*"
    # type of device to appy specs to
    data_devices:
      # a filter, saying to use all devices marked 'available'
      all: true

#
# RGW
#

cephadm_rgw_realm: ceph
cephadm_rgw_zone: use-east-1
cephadm_rgw_zonegroup: default
