# sample.all.yml

#
# Config
#

# TODO: Clarify how this works, probably with big warning notices and
# iterative examples that fully explan how the logic is expanded.  I
# did add some inline comments which should give context clues of how
# this is parameterized, so hopefully that is good enough for now.
cephadm_config:
  # Stage to run _AFTER_
  bootstrap:
    # ceph osd
    osd:
      # ceph osd erasure-code-profile set
      erasure-code-profile:
        # ceph osd erasure-code-profile set host-jerasure-2-1 ...
        - name: host-jerasure-2-1
          crush_failure_domain: host
          plugin: jerasure
          k: 2
          m: 1

        # ceph osd erasure-code-profile set host-jerasure-4-2 ...
        - name: host-jerasure-4-2
          crush_failure_domain: host
          plugin: jerasure
          k: 4
          m: 2

        # ceph osd erasure-code-profile set host-jerasure-8-3 ...
        - name: host-jerasure-8-3
          crush_failure_domain: host
          plugin: jerasure
          k: 8
          m: 3

        # ceph osd erasure-code-profile set rack-jerasure-8-3 ...
        - name: rack-jerasure-8-3
          crush_failure_domain: rack
          plugin: jerasure
          technique: reed_sol_van
          k: 8
          m: 3

#
# Dashboard
#

# Whether the dashboard should be deployed.
cephadm_dashboard: true

# The username and password to use for the administrative user
cephadm_dashboard_password: admin
cephadm_dashboard_username: admin

# The IP address and port(s) to use for the dashboard
cephadm_dashboard_server_addr: >-
  "{{ hostvars[inventory_hostname].ansible_default_ipv4.address }}"
cephadm_dashboard_server_port: 8080
cephadm_dashboard_server_port_ssl: 8443

# Add your own certs to the dashboard, if desired.  Maybe I should
# see about using LetsEncrypt to make/manage SSLs across the
# deployment.
cephadm_dashboard_ssl_crt: ""
cephadm_dashboard_ssl_key: ""

#
# Firewall
#

cephadm_firewall_firewalld: false

#
# Health
#

# Placeholder
cephadm_health_mon_check: ""

#
# Mon
#

# Placeholder
cephadm_mon: ""

#
# Network
#

#
# Values: ipv4, ipv6
cephadm_network_ip_version: ipv4

# The network interface assignments you want to make.  At minimum you
# must define your cluster interface, and the others will simply also
# use it.
cephadm_network_interface_cluster: eno1
cephadm_network_interface_public: "{{ cephadm_network_interface_cluster }}"

cephadm_network_interface_mgrs: "{{ cephadm_network_interface_cluster }}"
cephadm_network_interface_mons: "{{ cephadm_network_interface_public }}"
cephadm_network_interface_osds: "{{ cephadm_network_interface_cluster }}"
cephadm_network_interface_rgws: "{{ cephadm_network_interface_public }}"

#
# OSD
#

# https://docs.ceph.com/docs/octopus/cephadm/drivegroups/
cephadm_osd_drivegroups:
  # group name
  default:
    # hosts to match against
    host_pattern: "*"
    # type of device to appy specs to
    data_devices:
      # a filter, saying to use all devices marked 'available'
      all: true

#
# RGW
#

cephadm_rgw_realm: ceph
cephadm_rgw_zone: use-east-1
cephadm_rgw_zonegroup: default
