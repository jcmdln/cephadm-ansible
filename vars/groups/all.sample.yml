---
# all.sample.yml

#
# Ceph
#

ceph_origin: repository
ceph_release: nautilus
ceph_repository: community

#
# Network
#

network_private: 0.0.0.0/24
network_public: 0.0.1.0/24

network_interface_mon: eth1
network_interface_rgw: eth2

#
# Configuration
#

# Add or override settings that end up in /etc/ceph.conf
ceph_conf:
  something: ""

#
# MGRs
#

ceph_mgr_modules:
  - pg_autoscaler

# Run custom Ceph commands.
#
# Note that the command prefix 'ceph' will be added to somewhat prevent
# using this mechanism as an arbitrary vector for changes.
ceph_config_cmds:
  # Ensure pg_autoscale is enabled globally
  - osd set global osd_pool_default_pg_autoscale_mode on

  # Create a custom 8:2 erasure coding profile
  - osd erasure-code-profile set ec-8_2 k=8 m=2

#
# OSDs
#

# Allow an OSD to use up to 1 CPU (or thread) of the total available
# number of CPUs.
osd_container_max_cpu: "{{ (100 / ansible_processor_cores) / 100 }}"

# Allow an OSD to reserve 1GB of memory for each TB of storage it would
# be responsible for.
osd_container_max_mem: "{{ 1gb per 1tb }}"

# Hard-coded number of OSDs to create for each storage device based on
# their CRUSH device type.
osd_per_device:
  hdd: 1
  ssd: 2
  nvme: 4

# Below we may assign storage devices to an arbitrary number of groups
storage_devices:
  - name: mixed
    block:
      - /dev/sda
      - /dev/sdb
    blockdb:
      - /dev/nvme1n1
  - name: fast
    block:
      - /dev/nvme2n1
      - /dev/nvme3n1
      - /dev/nvme4n1
